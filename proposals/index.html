<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css"
    integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">

  <title>Project Proposals</title>

  <style>
    .no-margin {
      margin: 0;
    }
  </style>

</head>

<body class="text-justify" itemscope itemtype="http://schema.org/WebPage">
  <meta name="description" content="Presentations and public talks of Alex Serban">
  <meta name="relatedLink" content="xserban.github.io/papers">

  <div class="container-fluid">
    Back to
    <a href="https://xserban.github.io">Homepage</a>
    <br>
    <br>

    <div class="col-sm-8">
      <p>
        <!-- Previous students I worked with used machine learning for state sponsored malware authorship attribution.
        You can find an abstract <a target="alt"
          href="https://xserban.github.io/papers/pdf/MalwareStateAttribution.pdf">here</a>.
        A dataset for benchmarking algorithms on this task can be found <a target="alt"
          href="https://xserban.github.io/code/">here</a> and some code samples <a target="alt"
          href="https://github.com/cyber-research/APTAttribution">here</a>.

        <br>
        Others have worked on learning state representation for Atari games, which can be used to formally verify the
        behaviour of agents
        learning to play games. <a href="https://github.com/davidkerkkamp/representation-learning" target="_blank">Learn
          more here.</a>
        <br><br> -->



        <!-- If you are looking for a project in the area of safety or security for machine learning, feel free to <a
          href="https://www.cs.ru.nl/staff/Alexandru.Serban">contact me</a>. -->
        <!-- <br><br> -->
    </div>


    <br>
    <div class="col-sm-8">

      Past Student Projects:
      <br><br>


      <ul>
        <li>
          <a target="alt"
            href="https://www.cs.ru.nl/bachelors-theses/2021/Bart_Hofman___1018982___Automated_Malware_Attribution_-_Using_machine_learning_on_binaries.pdf">
            Automated
            Malware Attribution
          </a>- Bart Hofman, Radboud University (BSc. Thesis)
        </li>

        <li>
          <a target="alt" href="https://github.com/davidkerkkamp/representation-learning"> Learning State
            Representations
            for Formal Verification </a>- David Kerkkamp, Radboud University (Student Internship)
        </li>

        <li>
          <a target="alt" href="https://xserban.github.io/theses/wietze.pdf"> Where does this malware come from?
          </a> - Wietze Mulder, Radboud University (BSc. Thesis)

        </li>

        <li>
          <a target="alt" href="https://xserban.github.io/theses/b_coen_boot.pdf">
            Applying Supervised Learning on Malware Authorship Attribution
          </a> - Coen Boot, Radboud University (MSc. Thesis)
        </li>
      </ul>


    </div>



    <!-- Safe Properties Transfer in RL -->
    <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Safe Properties Transfer in RL
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Verify safety properties on a trained RL agent and transfer the knowledge to a new model.
                        Ideally the verified model is smaller.
                        In DL there are transfer techniques like distillation where knowledge from larger networks is
                        transferred
                        to smaller networks. Sometimes the other way around. It would be nice to see if safety
                        properties
                        can be transferred between RL agents.
                    </span>
                </p>
                <p class="no-margin">
                    <em>Papers:</em>
                    <ul>
                        <li>Transfer in Deep RL Using Successor Features and Generalised Policy Improvement. Successor
                            Features
                            and Generalised policy seem to be good knowledge transfer techniques in Deep RL. This paper
                            combines
                            both techniques.
                        </li>


                    </ul>
                </p>
                <br>
            </div>
        </div> -->
    <!-- Safe Properties Transfer in RL -->



    <!-- Generative Counter Example Adversarial Networks -->
    <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Generative Counter Example Adversarial Networks
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate the use of Generative Adversarial Networks to generate counter-examples for
                        probabilistic automatas.
                    </span>
                </p>
                <br>
            </div>
        </div> -->
    <!-- Generative Counter Example Adversarial Networks -->


    <!-- Virtual generative adversarial training / testing -->
    <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Virtual generative adversarial training / testing
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Train RL agents in a virtual environment with a generative adversarial network. Or test RL
                        agents (that already have some
                        verified properties (?)) using GAN to generate adversarial examples that make the properties
                        fail.
                    </span>
                </p>
                <br>
            </div>
        </div> -->
    <!-- Virtual generative adversarial training / testing -->


    <!-- Investigate how invariance affects adversarial examples. -->
    <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Investigate how invariance affects adversarial examples.
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate the impact of maxout (and other techniques used for invariance) on networks
                        sensitive to adversarial examples.
                    </span>
                </p>
                <p class="no-margin">
                    <em>Papers:</em>
                    <ul>
                        <li>Explaining and harnessing adversarial examples - I. Goodfellow etal.</li>
                        <li>Maxout Networks - I. Goodfellow etal.
                        </li>
                        <li>https://distill.pub/2016/deconv-checkerboard/</li>
                        <!-- strided convolutions and pooling operations -->


    </ul>
    </p>
    <br>
  </div>
  </div>
  <!-- Investigate how invariance affects adversarial examples. -->

  <!-- What can we tell about images for which we can not find adversarial examples? -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    What can we tell about images for which we can not find adversarial examples?
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        For some images it is genuinly harder to find adversarial examples.
                        Let's investigate why.
                    </span>
                </p>
                <p class="no-margin">
                    <em>Papers:</em>
                    <ul>
                        <li>Explaining and harnessing adversarial examples - I. Goodfellow etal.</li>
                    </ul>
                </p>
                <br>
            </div>
        </div> -->
  <!-- What can we tell about images for which we can not find adversarial examples? -->

  <!-- Manifold Distances -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Fast ways to compute/approximate manifold distances.
                </h6>

                <p class="no-margin">
                    <em>Papers:</em>
                    <ul>
                        <li>Joint manifold distance: a new approach to appearance based clustering</li>
                    </ul>
                </p>

                <br>
            </div>
        </div> -->
  <!-- Manifold Distances -->

  <!-- Fundamental reasons to adversarial examples -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Fundamental reasons to adversarial examples
                </h6>

                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate some of the fundamental conjectures on the existence of adversarial examples using
                        toy datasets.
                    </span>
                </p>
                <br>
            </div>
        </div> -->
  <!-- Fundamental reasons to adversarial examples -->



  <!-- Less Partially Observable Markov Decision Processes with Generative Adversarial Networks -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Less Partially Observable Markov Decision Processes with Generative Adversarial Networks
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate the use of GAN to learn a partially observable probability distribution and
                        generate further examples from it.
                        The generator network could be used to virtually train RL agents / make POMDPs less partially
                        observable
                    </span>
                </p>
                <br>
            </div>
        </div> -->
  <!-- Less Partially Observable Markov Decision Processes with Generative Adversarial Networks -->


  <!-- Generative Abstract MDP -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Generative Abstract MDP
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate the use of NN and GAN in learning a functions that abstract MDPs and makes property
                        check / agent learning easier.
                        A possible set-back is lack of data
                    </span>
                </p>

                <br>
            </div>
        </div> -->
  <!-- Generative Abstract MDP -->


  <!-- Boost safe agent learning with model checking -->
  <!-- <div class="row" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="col-sm-8" itemprop="learningResourceType">
                <h6 class="text-primary">
                    Boost safe agent learning with model checking
                </h6>


                <p class="no-margin">
                    <em>Description</em>:
                    <span>
                        Investigate if RL process can be somehow augmented to check safety properties while learning.
                    </span>
                </p>

                <br>
            </div>
        </div> -->
  <!-- Boost safe agent learning with model checking -->

  </div>

</body>

</html>